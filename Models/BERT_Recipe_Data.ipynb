{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Recipe_Data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AP15uzoW6T3",
        "colab_type": "text"
      },
      "source": [
        "# Verify GPU Availibility\n",
        "## if Error:\n",
        "## Edit -> Notebook Settings -> Select Hardware Accelerator as \"GPU\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEfSbAA4QHas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4aTLJqkXPBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install \n",
        "!pip install pytorch-pretrained-bert pytorch-nlp\n",
        "\n",
        "# BERT imports\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertAdam, BertConfig\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "from pytorch_pretrained_bert import WEIGHTS_NAME, CONFIG_NAME\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial.distance import cosine\n",
        "import os\n",
        "% matplotlib inline\n",
        "\n",
        "# specify GPU device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bgPN8iHhYWA",
        "colab_type": "text"
      },
      "source": [
        "# Import Data From Local Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l48wRKg-hZ0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GimT_r_JPIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etTqP2nAjRbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('/content/drive/My Drive/baking_data_title_ingredients.pickle','rb') as f:\n",
        "  baking_data = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/My Drive/nutritional_info.pickle','rb') as f:\n",
        "  nutritional_df = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ3FzN6ubEaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_token_seq_to_string(token_seq, token_num_to_str):\n",
        "  return ' '.join([token_num_to_str[num] if num in token_num_to_str else num for num in token_seq])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fmy7yvxNy94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get the baking info\n",
        "\n",
        "#get the mask\n",
        "health_mask = baking_data[0].id.isin(nutritional_df.id)\n",
        "health_indices = [i for i,val in enumerate(health_mask) if val==True]\n",
        "\n",
        "print(len(health_indices))\n",
        "\n",
        "#baking dataframe\n",
        "df = baking_data[0][health_mask]\n",
        "\n",
        "print(len(df))\n",
        "\n",
        "#training recipe ids\n",
        "baking_ids = df.id.values\n",
        "\n",
        "#list of strings representing each recipe\n",
        "baking_strings = ['[CLS] ' + item.replace('--|||--', '[SEP]').replace('||', '[SEP]') + ' [SEP]' for i,item in enumerate(baking_data[1]) if i in health_indices]\n",
        "\n",
        "#dictionary mapping imported tokens ids to token strings\n",
        "token_num_to_str = baking_data[7]\n",
        "\n",
        "#list of lists where each list is of token ids with some missing\n",
        "#test_baking_tokens_missing = [item[0] for item in baking_data[4]]\n",
        "#test_baking_strings_missing = ['[CLS] ' + convert_token_seq_to_string(r, token_num_to_str).replace('||', '[SEP]').replace('MASK', '[MASK]') + ' [SEP]' for r in test_baking_tokens_missing]\n",
        "\n",
        "#list of lists where each list is of token ids\n",
        "#test_baking_tokens_full = baking_data[3]\n",
        "#test_baking_strings_full = ['[CLS] ' + convert_token_seq_to_string(r, token_num_to_str).replace('||', '[SEP]') + ' [SEP]' for r in test_baking_tokens_full]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luxQCjY8bKM1",
        "colab_type": "text"
      },
      "source": [
        "# Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QElO35oMZDFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize with BERT tokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "tokenized_recipes = [tokenizer.tokenize(r) for r in baking_strings]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqpPqt_0uZ55",
        "colab_type": "text"
      },
      "source": [
        "# Pad Tokenized Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgWWTDwxouv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length = max([len(seq) for seq in tokenized_recipes])\n",
        "print(max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnZpbcyPl96g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the maximum sequence length. \n",
        "# NOTE: THIS SHOULD BE SET TO ABOVE THE MAX IN THE CELL ABOVE\n",
        "MAX_LEN = max_length\n",
        "\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_recipes],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrPmMX4gnAN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "masked_lm_labels = []\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  lm_labels = [(-1 if i==0 else i) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "  masked_lm_labels.append(lm_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu1KbeXDtay2",
        "colab_type": "text"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EetQ_Ase0LYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bvBL_rX5tCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "#train_inputs, _, train_masks, _, train_labels, _ = train_test_split(input_ids, attention_masks, masked_lm_labels, random_state=2018, test_size=1)\n",
        "                                             \n",
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "train_inputs = torch.tensor(input_ids)\n",
        "train_masks = torch.tensor(attention_masks)\n",
        "train_lm_labels = torch.tensor(masked_lm_labels)\n",
        "\n",
        "# Select a batch size for training. \n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader \n",
        "train_data = TensorDataset(train_inputs, train_masks, train_lm_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaohKft06XNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "#move model to gpu\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrXo89Sj2wn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# BERT fine-tuning parameters\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=2e-5,\n",
        "                     warmup=.1)\n",
        "  \n",
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "# Number of training epochs \n",
        "epochs = 2\n",
        "\n",
        "# BERT training loop\n",
        "for _ in trange(epochs, desc=\"Epoch\"):  \n",
        "  \n",
        "  ## TRAINING\n",
        "  \n",
        "  # Set our model to training mode\n",
        "  model.train()  \n",
        "  # Tracking variables\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  # Train the data for one epoch\n",
        "\n",
        "  tot = len(train_dataloader)\n",
        "\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    if step % 50 == 0:\n",
        "      print('Step %s of %s'%(step, tot))\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_lm_labels = batch\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, masked_lm_labels=b_lm_labels)\n",
        "    train_loss_set.append(loss.item())    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "\n",
        "# plot training performance\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(train_loss_set)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mijvq17_a3vx",
        "colab_type": "text"
      },
      "source": [
        "# Getting Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi7BajIamxc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embeddings(input_ids, model, gpu=True):\n",
        "  if gpu:\n",
        "    input_ids = input_ids.to(device)\n",
        "  outputs = model(input_ids)\n",
        "  embeddings = outputs[:,0,:]\n",
        "  embeddings_n = embeddings.cpu().detach().numpy()\n",
        "  return embeddings_n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofYMLGs1AKTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "num_batches = len(train_inputs) // batch_size + 1\n",
        "\n",
        "for b in range(num_batches):\n",
        "  if b % 10 == 0:\n",
        "    print('Batch: %s'%(b+1))\n",
        "  curr_batch = train_inputs[batch_size*b:batch_size*(b+1)]\n",
        "  if b==0:\n",
        "    embeddings = get_embeddings(curr_batch, model)\n",
        "  else:\n",
        "    curr_embeddings = get_embeddings(curr_batch, model)\n",
        "    embeddings = np.concatenate((embeddings, curr_embeddings), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWXeIB8VV8Nn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT4ZcY4VtHeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "num = embeddings.shape[0]\n",
        "\n",
        "start = time()\n",
        "\n",
        "sim_mtx = np.zeros((num, num))\n",
        "\n",
        "for idx1 in range(num):\n",
        "  if idx1 % 100 == 0:\n",
        "    print(idx1)\n",
        "  emb1 = embeddings[idx1]\n",
        "  for idx2 in range(idx1+1, num):\n",
        "    emb2 = embeddings[idx2]\n",
        "    cos_sim = 1 - cosine(emb1, emb2)\n",
        "\n",
        "    sim_mtx[idx1, idx2] = cos_sim\n",
        "    sim_mtx[idx2, idx1] = cos_sim\n",
        "\n",
        "end = time()\n",
        "print(end-start)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3G2_ZrH3OQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for idx in range(20):\n",
        "  print('Recipe:')\n",
        "  print(df[df.id == baking_ids[idx]].title.iloc[0])\n",
        "  print('Similar Recipes:')\n",
        "  row = sim_mtx[idx]\n",
        "  most_similar_inds = np.argpartition(row, -6)[-6:]\n",
        "  print(most_similar_inds)\n",
        "  most_similar_ids = [baking_ids[i] for i in most_similar_inds]\n",
        "  print(df[df.id.isin(most_similar_ids)].title.values)\n",
        "  print('--------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLobCJUUBuhr",
        "colab_type": "text"
      },
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaTOh7ikDSrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If we save using the predefined names, we can load using `from_pretrained`\n",
        "output_dir = '/content/drive/My Drive/'\n",
        "output_model_file = os.path.join(output_dir, 'trained_bert_model')\n",
        "output_config_file = os.path.join(output_dir, CONFIG_NAME)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxFKTWmyB3Ws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), output_model_file)\n",
        "model.config.to_json_file(output_config_file)\n",
        "tokenizer.save_vocabulary(output_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD5aGBEeC0ou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}