{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BidirectionalLSTM_Recipe_Data_Keras.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyOo3bb9QWmtsJ/Ah04kRghL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"1f2_9zuVlxcL","colab_type":"code","colab":{}},"source":["'''\n","Bi Directional LSTM Autoencoder\n","'''\n","from numpy import array\n","import numpy as np\n","import keras\n","import tensorflow as tf\n","from keras.models import Sequential\n","from keras.models import Model\n","from keras.layers import LSTM,GRU,Bidirectional\n","from keras.layers import Dense\n","from keras.layers import RepeatVector\n","from keras.layers import TimeDistributed\n","from keras.preprocessing import sequence\n","import sklearn.metrics\n","import os\n","import random"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oRYDEsEC2moD","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"D_ESJCTJ2tym","colab_type":"code","colab":{}},"source":["################################\n","################################\n","################################\n","#Load Data\n","\n","#Lang class: create unique word index dictionary\n","SOS_token = 0\n","EOS_token = 1\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","        self.n_words = 2  # Count SOS and EOS\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1\n","\n","#import data\n","import pickle\n","with open('/content/drive/My Drive/CS 263 Project/Final Project Data/baking_data_title_ingredients.pickle','rb') as f:\n","    baking_data = pickle.load(f)\n","\n","joined_limited = baking_data[0]\n","cleaned_recipes = baking_data[1]\n","numerical_tokens_train = baking_data[2]\n","numerical_tokens_test = baking_data[3]\n","numerical_tokens_test_masked = baking_data[4]\n","IDs_train = baking_data[5]\n","IDs_test = baking_data[6]\n","#vocabulary_dict = baking_data[7]\n","#vocabulary = baking_data[8]\n","cleaned_recipes_IDs = baking_data[9]\n","\n","MAX_LENGTH = max([len(s) for s in numerical_tokens_train])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WOOvVXibpemP","colab_type":"code","colab":{}},"source":["#cut data down to recipes len 100\n","#data = numerical_tokens_train\n","MAX_LENGTH = 75\n","#prep data -- subset, make into list, normalize and create vocab\n","def prep(dta):\n","    temp = dta\n","    temp = [temp[i] for i in range(0,len(temp)) if len(temp[i].split())<= MAX_LENGTH]\n","    v = Lang('vocab')\n","    for d in temp:\n","        v.addSentence(d)\n","    print(\"Counted words: \",v.n_words)\n","    return temp,v\n","\n","data_strings, vocabulary = prep(cleaned_recipes)\n","titles = [joined_limited.loc[i,\"title\"] for i in range(0,len(cleaned_recipes)) if len(cleaned_recipes[i].split())<= MAX_LENGTH]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"813D-wEqBp21","colab_type":"code","colab":{}},"source":["data = [[vocabulary.word2index[w] for w in s.split(\" \")] for s in data_strings]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UWDQBdL9zPQY","colab_type":"code","colab":{}},"source":["#pad \n","data = sequence.pad_sequences(data, maxlen=MAX_LENGTH,dtype='int32',value=0.0,padding=\"post\")\n","\n","# reshape input into [samples, timesteps, features]\n","n_examples = len(data)\n","n_words = MAX_LENGTH\n","data = data.reshape((n_examples, n_words, 1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iMaHI1egxAVY","colab_type":"code","colab":{}},"source":["################################\n","################################\n","################################\n","#Model\n","\n","#define EncoderDecoder Layers\n","hidden_size = 512\n","EncoderDecoder = Sequential()\n","EncoderDecoder.add(Bidirectional(LSTM(hidden_size, activation='sigmoid', input_shape=(n_words,1))))\n","EncoderDecoder.add(RepeatVector(n_words))\n","EncoderDecoder.add(Bidirectional(LSTM(hidden_size, activation='sigmoid', return_sequences=True)))\n","EncoderDecoder.add(TimeDistributed(Dense(vocabulary.n_words ,activation=\"softmax\")))\n","\n","#define optimizer and loss\n","optim = keras.optimizers.RMSprop(learning_rate=0.001)#,decay=0.99)\n","EncoderDecoder.compile(optimizer=optim,loss=\"sparse_categorical_crossentropy\",metrics=['sparse_categorical_accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v5_5aMYCzUaV","colab_type":"code","colab":{}},"source":["#fit EncoderDecoder\n","history = EncoderDecoder.fit(x=data, y=data, epochs=10, verbose=1,batch_size=100,shuffle=True,validation_split = 0.05)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7njsYcD3f7uR","colab_type":"code","colab":{}},"source":["from matplotlib import pyplot as plt\n","plt.plot(history.history['sparse_categorical_accuracy'])\n","plt.plot(history.history['val_sparse_categorical_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6LODAj2aZ7Hr","colab_type":"code","colab":{}},"source":["# save model and architecture to single file\n","EncoderDecoder.save(\"/content/drive/My Drive/CS 263 Project/Saved Models/BiLSTM_EncoderDecoder.h5\")\n","Encoder.save(\"/content/drive/My Drive/CS 263 Project/Saved Models/BiLSTM_Encoder.h5\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cqf-8UH3aRMH","colab_type":"code","colab":{}},"source":["#load model \n","from keras.models import load_model\n","EncoderDecoder = load_model('/content/drive/My Drive/CS 263 Project/Saved Models/BiLSTM_EncoderDecoder.h5')\n","EncoderDecoder.summary()\n","\n","Encoder = load_model('/content/drive/My Drive/CS 263 Project/Saved Models/BiLSTM_Encoder.h5')\n","Encoder.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R8qd6tt7yfF2","colab_type":"code","colab":{}},"source":["#get embeddings\n","# make encoder LSTM output layer \n","Encoder = Model(inputs=EncoderDecoder.inputs, outputs=EncoderDecoder.layers[0].output)\n","# get the feature vector for the input sequence\n","embeddings = Encoder.predict(data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HTq4w5hyzAqi","colab_type":"code","colab":{}},"source":["#get embeddings for recipies with nutritional info\n","with open('/content/drive/My Drive/CS 263 Project/Final Project Data/nutritional_info.pickle','rb') as f:\n","    nutritional_info = pickle.load(f)\n","\n","embeddings_IDs = [cleaned_recipes_IDs[i] for i in range(0,len(cleaned_recipes_IDs)) if len(cleaned_recipes[i].split())<= MAX_LENGTH]\n","nutri_embeddings_indices = [i for i, val in enumerate(embeddings_IDs) if val in nutritional_info[\"id\"].tolist()]\n","\n","nutri_embeddings_IDs = [embeddings_IDs[i] for i in nutri_embeddings_indices]\n","nutri_embeddings = [embeddings[i] for i in nutri_embeddings_indices]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SuuA-8rq0Z3I","colab_type":"code","colab":{}},"source":["#create matrix of cosine similarities for recipes with nutritional info and save with IDs\n","cosine_similarities = sklearn.metrics.pairwise.cosine_similarity(nutri_embeddings, Y=None, dense_output=False)\n","print(cosine_similarities)\n","\n","import pickle\n","similarity_pickle = [nutri_embeddings,nutri_embeddings_IDs,cosine_similarities]\n","with open('/content/drive/My Drive/CS 263 Project/Final Project Data/BiLSTM_similarity.pickle', 'wb') as f:\n","    pickle.dump(similarity_pickle, f)"],"execution_count":0,"outputs":[]}]}
